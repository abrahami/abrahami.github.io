---
---

@string{aps = {American Physical Society,}}

@inproceedings{bringer2019osprey,
  abbr={DEEM},
  title={Osprey: Weak supervision of imbalanced extraction problems without code},
  author={Bringer, Eran and Israeli, Abraham and Shoham, Yoav and Ratner, Alex and R{\'e}, Christopher},
  booktitle={Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning},
  pages={1--11},
  year={2019},
  abstract={Supervised methods are commonly used for machine-learning based applications but require expensive labeled dataset creation and maintenance. Increasingly, practitioners employ weak supervision approaches, where training labels are programmatically generated in higher-level but noisier ways. 
            However, these approaches require domain experts with programming skills. Additionally, highly imbalanced data is often a significant practical challenge for these approaches.
            In this work, we propose Osprey, a weak-supervision system suited for highly-imbalanced data, built on top of the Snorkel framework.
            In order to support non-coders, the programmatic labeling is decoupled into a code layer and a configuration one. This decoupling enables a rapid development of end-to-end systems by encoding the business logic into the configuration layer. 
            We apply the resulting system on highly-imbalanced (0.05{\%} positive) social-media data using a synthetic data rebalancing and augmentation approach, and a novel technique of ensembling a generative model over the legacy rules with a learned discriminative model.
            We demonstrate how an existing rule-based model can be transformed easily into a weakly-supervised one. 
            For 3 relation extraction applications based on real-world deployments at Intel, we show that with a fraction of the cost, we achieve gains of 18.5 precision points and 28.5 coverage points over prior traditionally supervised and rules-based approaches.},
  url={https://dl.acm.org/doi/abs/10.1145/3329486.3329492},
  html={https://dl.acm.org/doi/abs/10.1145/3329486.3329492},
  pdf={Osprey_2019.pdf}
}

@article{israeli2019constraint,
  abbr={ESWA},
  title={Constraint learning based gradient boosting trees},
  author={Israeli, Abraham and Rokach, Lior and Shabtai, Asaf},
  journal={Expert Systems with Applications},
  volume={128},
  pages={287--300},
  year={2019},
  publisher={Elsevier},
  abstract={Predictive regression models aim to find the most accurate solution to a given problem, often without any constraints related to the model’s predicted values. Such constraints have been used in prior research where they have been applied to a subpopulation within the training dataset which is of greater interest and importance. In this research we introduce a new setting of regression problems, in which each instance can be assigned a different constraint, defined based on the value of the target (predicted) attribute. 
            The new use of constraints is taken into account and incorporated into the learning process, and is also considered when evaluating the induced model. We propose two algorithms which are modifications to the regression boosting method. There are two advantages of the proposed algorithms: they are not dependent on the base learner used during the learning process, and they can be adopted by any boosting technique. We implemented the algorithms by modifying the gradient boosting trees (GBT) model, and we also introduced two measures for evaluating the models that were trained to solve the constraint problems. 
            We compared the proposed algorithms to three baseline algorithms using four real-life datasets. Due to the algorithms’ focus on satisfying the constraints, in most cases the results showed significant improvement in the constraint-related measures, with just a minimal effect on the general prediction error. The main impact of the proposed approach is in its ability to derive a model with a higher level of assurance for specific cases of interest (i.e., the constrained cases). This is extremely important and has great significance in various use cases and expert and intelligent systems, particularly critical systems, such as critical healthcare systems (e.g., when predicting blood pressure or blood sugar level), safety systems (e.g., when aiming to estimate the distance of cars or airplanes from other objects), or critical industrial systems (e.g., require to estimate their usability along time). 
            In each of these cases, there is a subpopulation of all instances that is of greater interest to the expert or system, and the sensitivity of the model’s error changes according to the real value of the predicted feature. For example, for a subpopulation of patients (e.g., patients under the age of eight, or patients known to be at risk), physicians often require a sensitive model that accurately predicts blood pressure values.},
  url={https://www.sciencedirect.com/science/article/pii/S0957417419301630},
  html={https://www.sciencedirect.com/science/article/pii/S0957417419301630},
  pdf={constraint_learning_2019.pdf}
}

@inproceedings{israeli2021idc,
  abbr={WANLP},
  title={The idc system for sentiment classification and sarcasm detection in Arabic},
  author={Israeli, Abraham and Nahum, Yotam and Fine, Shai and Bar, Kfir},
  booktitle={Proceedings of the Sixth Arabic Natural Language Processing Workshop},
  pages={370--375},
  year={2021},
  abstract={entiment classification and sarcasm detection attract a lot of attention by the NLP research community. However, solving these two problems in Arabic and on the basis of social network data (i.e., Twitter) is still of lower interest. In this paper we present designated solutions for sentiment classification and sarcasm detection tasks that were introduced as part of a shared task by Abu Farha et al. (2021). 
            We adjust the existing state-of-the-art transformer pretrained models for our needs. In addition, we use a variety of machine-learning techniques such as down-sampling, augmentation, bagging, and usage of meta-features to improve the models performance. We achieve an F1-score of 0.75 over the sentiment classification problem where the F1-score is calculated over the positive and negative classes (the neutral class is not taken into account). 
            We achieve an F1-score of 0.66 over the sarcasm detection problem where the F1-score is calculated over the sarcastic class only. In both cases, the above reported results are evaluated over the ArSarcasm-v2–an extended dataset of the ArSarcasm (Farha and Magdy, 2020) that was introduced as part of the shared task. This reflects an improvement to the state-of-the-art results in both tasks.},
  url={https://aclanthology.org/2021.wanlp-1.48/},
  html={https://aclanthology.org/2021.wanlp-1.48/},
  pdf={wanlp_2021.pdf}
}

@inproceedings{israeli2022free,
  abbr={WOAH},
  title={Free speech or Free Hate Speech? Analyzing the Proliferation of Hate Speech in Parler},
  author={Israeli, Abraham and Tsur, Oren},
  booktitle={Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)},
  pages={109--121},
  year={2022},
  selected={true},
  abstract={Social platforms such as Gab and Parler, branded as ‘free-speech’ networks, have seen a significant growth of their user base in recent years. This popularity is mainly attributed to the stricter moderation enforced by mainstream platforms such as Twitter, Facebook, and Reddit.In this work we provide the first large scale analysis of hate-speech on Parler. 
            We experiment with an array of algorithms for hate-speech detection, demonstrating limitations of transfer learning in that domain, given the illusive and ever changing nature of the ways hate-speech is delivered. In order to improve classification accuracy we annotated 10K Parler posts, which we use to fine-tune a BERT classifier. Classification of individual posts is then leveraged for the classification of millions of users via label propagation over the social network. 
            Classifying users by their propensity to disseminate hate, we find that hate mongers make 16.1{\%} of Parler active users, and that they have distinct characteristics comparing to other user groups. We further complement our analysis by comparing the trends observed in Parler to those found in Gab. To the best of our knowledge, this is among the first works to analyze hate speech in Parler in a quantitative manner and on the user level.},
  url={https://aclanthology.org/2022.woah-1.11/},
  html={https://aclanthology.org/2022.woah-1.11/},
  pdf={parler_2022.pdf}
}

@inproceedings{israeli2022must,
  abbr={WWW},
  title={This Must Be the Place: Predicting Engagement of Online Communities in a Large-scale Distributed Campaign},
  author={Israeli, Abraham and Kremiansky, Alexander and Tsur, Oren},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={1673--1684},
  year={2022},
  selected={true},
  abstract={Understanding collective decision making at a large-scale, and elucidating how community organization and community dynamics shape collective behavior are at the heart of social science research. In this work we study the behavior of thousands of communities with millions of active members. We define a novel task: predicting which community will undertake an unexpected, large-scale, distributed campaign. To this end, we develop a hybrid model, combining textual cues, community meta-data, and structural properties. We show how this multi-faceted model can accurately predict large-scale collective decision-making in a distributed environment. We demonstrate the applicability of our model through Reddit’s r/place – a large-scale online experiment in which millions of users, self-organized in thousands of communities, clashed and collaborated in an effort to realize their agenda.
            Our hybrid model achieves a high F1 prediction score of 0.826. We find that coarse meta-features are as important for prediction accuracy as fine-grained textual cues, while explicit structural features play a smaller role. Interpreting our model, we provide and support various social insights about the unique characteristics of the communities that participated in the r/place experiment.
            Our results and analysis shed light on the complex social dynamics that drive collective behavior, and on the factors that propel user coordination. The scale and the unique conditions of the r/place experiment suggest that our findings may apply in broader contexts, such as online activism, (countering) the spread of hate speech and reducing political polarization. The broader applicability of the model is demonstrated through an extensive analysis of the WallStreetBets community, their role in r/place and four years later, in the GameStop short squeeze campaign of 2021.},
  url={https://dl.acm.org/doi/abs/10.1145/3485447.3512238},
  html={https://dl.acm.org/doi/abs/10.1145/3485447.3512238},
  pdf={www_rplace_2022.pdf}
}

@article{israeli2022unsupervised,
  abbr={ESWA},
  title={Unsupervised discovery of non-trivial similarities between online communities},
  author={Israeli, Abraham and Cohen, Shani and Tsur, Oren},
  journal={Expert Systems with Applications},
  volume={206},
  pages={117900},
  year={2022},
  publisher={Elsevier},
  abstract={Language is used differently across communities. The differences may be manifested in vocabulary, style, and semantics. These differences enable the exploration of nuanced similarities and differences between communities. 
            In this work, we introduce C3 — a novel unsupervised approach for community comparison. C3 creates contextual pairwise representations by aligning communities and tuning word embeddings according to both the lexical context and the social context reflected by the community’s structure and the community engagement patterns. 
            Specifically, C3 takes into account the semantic relations between pairs of words, reflected by the embeddings model of each community, and leverages the social context and users’ role in their community to calculate a similarity measure between community pairs. 
            C3 is evaluated over a dataset of 1565 active Reddit communities, comparing results against three competitive models. We show through an array of experiments and validations that C3 recovers nuanced and not-trivial similarities between communities that are not captured by any of the competitive models. 
            We complement the quantitative results with a qualitative analysis, discussing recovered non-trivial similarities between community pairs such as: opiates and adhd, babyBumps and depression, wallStreetBets and sandersForPresident, all of which are recovered by C3 but not by any of the other models. This qualitative analysis demonstrates the exploratory power of our model.},
  url={https://www.sciencedirect.com/science/article/pii/S0957417422011472},
  html={https://www.sciencedirect.com/science/article/pii/S0957417422011472},
  pdf={C3_2022.pdf}
}

@inproceedings{israeli-etal-2022-love-love,
    abbr={NSURL},
    title = {Love Me, Love Me Not: Human-Directed Sentiment Analysis in {A}rabic},
    author = {Israeli, Abraham  and Naaman, Aviv  and Nahum, Yotam  and Assi, Razan  and Fine, Shai  and Bar, Kfir},
    booktitle = {Proceedings of the Third International Workshop on NLP Solutions for Under Resourced Languages (NSURL 2022) co-located with ICNLSP 2022},
    month = {dec},
    year = {2022},
    address = {Trento, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.nsurl-1.4},
    pages = {22--30},
    abstract={Gauging the emotions people have toward a specific topic is a major natural language processing task, supporting various applications. The topic can be either an abstract idea (e.g., religion) or a service/product that someone writes a review about. 
              In this work, we define the topic to be a person who writes a post on a social media platform. More precisely, we introduce a new sentiment analysis task for detecting the sentiment that is expressed by a user toward another user in a discussion thread. 
              Modeling this new task may be beneficial for various applications, including hate-speech detection, and cyber-bullying mitigation. 
              We focus on Arabic, which is one of the most popular spoken languages worldwide, divided into various dialects that are used on social media platforms. 
              We compose a corpus of 3,500 pairs of tweets, with the second tweet being a response to the first one, and manually annotate them for the sentiment that is expressed in the response toward the author of the main tweet. 
              We train several baseline models and discuss their results and limitations. The best classification result that we recorded is 82{\%} F1 score. We release the corpus alongside the best-performing model},
    url={https://aclanthology.org/2022.nsurl-1.4/},
    html={https://aclanthology.org/2022.nsurl-1.4/},
    pdf={nsurl_arabic_2022.pdf}
}

@article{israeli2023flying,
  abbr={ICWSM},
  title={With Flying Colors: Predicting Community Success in Large-scale Collaborative Campaigns},
  author={Israeli, Abraham and Tsur, Oren},
  journal={arXiv preprint arXiv:2307.09650},
  month = {july},
  year={2023},
  abstract={Online communities develop unique characteristics, establish social norms, and exhibit distinct dynamics among their members. 
            Activity in online communities often results in concrete ``off-line'' actions with a broad societal impact (e.g., political street protests and norms related to sexual misconduct). 
            While community dynamics, information diffusion, and online collaborations have been widely studied in the past two decades, quantitative studies that measure the effectiveness of online communities in promoting their agenda are scarce. 
            In this work, we study the correspondence between the effectiveness of a community, measured by its success level in a competitive online campaign, and the underlying dynamics between its members. To this end, we define a novel task: predicting the success level of online communities in Reddit's r/place - a large-scale distributed experiment that required collaboration between community members. 
            We consider an array of definitions for success level; each is geared toward different aspects of collaborative achievement. We experiment with several hybrid models, combining various types of features. Our models significantly outperform all baseline models over all definitions of `success level'. 
            Analysis of the results and the factors that contribute to the success of coordinated campaigns can provide a better understanding of the resilience or the vulnerability of communities to online social threats such as election interference or anti-science trends. 
            We make all data used for this study publicly available for further research.},
  url={https://arxiv.org/pdf/2307.09650.pdf},
  html={https://arxiv.org/pdf/2307.09650.pdf},
  pdf={icwsm_rplace_2023.pdf}              
}

@article{marzea2024aggreghate,
  abbr={arXiv},
  title={AggregHate: An Efficient Aggregative Approach for the Detection of Hatemongers on Social Platforms},
  author={Marzea, Tom and Israeli, Abraham and Tsur, Oren},
  journal={arXiv preprint arXiv:2409.14464},
  month = {September},
  year={2024},
  abstract={Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. 
  Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. 
  While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. 
  In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. 
  We evaluate our methods on three unique datasets X (Twitter), Gab, and Parler showing that a processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. 
  Our method can be then used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as inform intervention measures. 
  Moreover, our approach is highly efficient even for very large datasets and networks.},
  url={https://arxiv.org/pdf/2409.14464},
  html={https://arxiv.org/pdf/2409.14464},
  pdf={aggrehate_2024.pdf}            
}

@article{ivey2024real,
  abbr={arXiv},
  title={Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue},
  author={Ivey, Johnathan and Kumar, Shivani and Liu, Jiayu and Shen, Hua and Rakshit, Sushrita and Raju, Rohan and Zhang, Haotian and Ananthasubramaniam, Aparna and Kim, Junghwan and Yi, Bowen and Dustin, Wright and Israeli, Abraham and Giovanni Møller, Anders and Zhang, Lechen and Jurgens, David},
  journal={arXiv preprint arXiv:2409.08330},
  month = {September},
  year={2024},
  abstract={Studying and building datasets for dialogue tasks is both expensive 
            and time-consuming due to the need to recruit, train, and collect data from study participants. 
            In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. 
            However, to what extent do LLM-based simulations reflect human dialogues? 
            In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. 
            Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. 
            Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. 
            Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.},
  url={https://arxiv.org/pdf/2409.08330},
  html={https://arxiv.org/pdf/2409.08330},
  pdf={real_or_robotic_2024.pdf}            
}

@article{israeli2024diaset,
  abbr={LREC-COLING},
  title={DiaSet: An Annotated Dataset of Arabic Conversations},
  author={Israeli, Abraham and Naaman, Aviv and Maduel, Guy and Makhoul, Rawaa and Qaraeen, Dana and Ejmail, Amir and Lisnanskey, Dina and Jubran, Julian and Fine, Shai and Bar, Kfir},
  journal={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month = {May},
  year={2024},
  abstract={We introduce DiaSet, a novel dataset of dialectical Arabic speech, manually transcribed and annotated for two
            specific downstream tasks: sentiment analysis and named entity recognition. The dataset encapsulates the Palestine
            dialect, predominantly spoken in Palestine, Israel, and Jordan. Our dataset incorporates authentic conversations
            between YouTube influencers and their respective guests. Furthermore, we have enriched the dataset with simulated
            conversations initiated by inviting participants from various locales within the said regions. The participants were
            encouraged to engage in dialogues with our interviewer. Overall, DiaSet consists of 644.8K tokens and 23.2K
            annotated instances. Uniform writing standards were upheld during the transcription process. Additionally, we
            established baseline models by leveraging some of the pre-existing Arabic BERT language models, showcasing the
            potential applications and efficiencies of our dataset. We make DiaSet publicly available for further research.},
  url={https://aclanthology.org/2024.lrec-main.436.pdf},
  html={https://aclanthology.org/2024.lrec-main.436.pdf},
  pdf={diaset_lrec_2024.pdf}              
}

